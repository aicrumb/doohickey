{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd06fad02e5d4deaacd65bbdebffc87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e96b6ebae4cc4d0aad4ca11c29b1a476",
              "IPY_MODEL_927bf2d0acac4ab8b953cb4be9e7d7af",
              "IPY_MODEL_96cd11a92fa34606b048f6c3bf1e8499",
              "IPY_MODEL_70eb672625d24395bbd6282dac3e9606"
            ],
            "layout": "IPY_MODEL_e64ad3167aa145d38456614e654f6719"
          }
        },
        "e96b6ebae4cc4d0aad4ca11c29b1a476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5ba4f7802f40949931b5928e5836a7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_35fc4d4271154bdaac800e6bbea00060",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "927bf2d0acac4ab8b953cb4be9e7d7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1a025d981a904e31949f4e7a823681d9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6bfe7815677240d892f4a5fd4abe1dba",
            "value": ""
          }
        },
        "96cd11a92fa34606b048f6c3bf1e8499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c7e19dbcd4664343b6eb01790b80a09f",
            "style": "IPY_MODEL_def51c2c3e33400ebf210d9b3ffc5d39",
            "tooltip": ""
          }
        },
        "70eb672625d24395bbd6282dac3e9606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3492301953114283b61376e447699419",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dffdf2082a8d4e4d8780e5fd007a0db7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e64ad3167aa145d38456614e654f6719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2b5ba4f7802f40949931b5928e5836a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fc4d4271154bdaac800e6bbea00060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a025d981a904e31949f4e7a823681d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bfe7815677240d892f4a5fd4abe1dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e19dbcd4664343b6eb01790b80a09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def51c2c3e33400ebf210d9b3ffc5d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3492301953114283b61376e447699419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dffdf2082a8d4e4d8780e5fd007a0db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Doohickey Gamma 0.5.0\n",
        "---\n",
        "[aicrumb](https://twitter.com/aicrumb)"
      ],
      "metadata": {
        "id": "e-MwsFWQ7CCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "bd06fad02e5d4deaacd65bbdebffc87c",
            "e96b6ebae4cc4d0aad4ca11c29b1a476",
            "927bf2d0acac4ab8b953cb4be9e7d7af",
            "96cd11a92fa34606b048f6c3bf1e8499",
            "70eb672625d24395bbd6282dac3e9606",
            "e64ad3167aa145d38456614e654f6719",
            "2b5ba4f7802f40949931b5928e5836a7",
            "35fc4d4271154bdaac800e6bbea00060",
            "1a025d981a904e31949f4e7a823681d9",
            "6bfe7815677240d892f4a5fd4abe1dba",
            "c7e19dbcd4664343b6eb01790b80a09f",
            "def51c2c3e33400ebf210d9b3ffc5d39",
            "3492301953114283b61376e447699419",
            "dffdf2082a8d4e4d8780e5fd007a0db7"
          ]
        },
        "cellView": "form",
        "id": "iUE0MenZGdSn",
        "outputId": "0e074605-92cd-46d5-c199-f4bc8dbc843c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries already installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd06fad02e5d4deaacd65bbdebffc87c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Install required libraries / Log in to the ðŸ¤— hub\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "if not os.path.exists(\"installed.txt\"):\n",
        "    # red lines, it's fines, that's what i always say\n",
        "    !pip install transformers diffusers lpips -q\n",
        "    !pip install git+https://github.com/openai/CLIP -q\n",
        "    !pip install open_clip_torch omegaconf -q\n",
        "    !pip install wget -q\n",
        "    !sudo apt-get install git-lfs\n",
        "    !cat \"test\" > installed.txt\n",
        "    !mkdir /content/output\n",
        "    print(\"Installed libraries\")\n",
        "    time.sleep(1) # just so that the user can see a glimpse of the print to know it went succesfuly\n",
        "    clear_output(wait=False)\n",
        "else:\n",
        "    print(\"Libraries already installed.\")\n",
        "\n",
        "#@markdown token param not tested, if left blank will let you log in with the widget\n",
        "token = \"\" #@param {type:\"string\"}\n",
        "if token != \"\":\n",
        "    !mkdir -p ~./huggingface\n",
        "    !echo -n \"$token\" > ~/.huggingface/token\n",
        "else:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries / models\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers import LMSDiscreteScheduler, DDIMScheduler, KarrasVeScheduler, PNDMScheduler, DDPMScheduler\n",
        "from IPython.display import Image, display\n",
        "from tqdm.auto import tqdm, trange\n",
        "from torch import autocast\n",
        "import PIL.Image as PImage\n",
        "import PIL\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as f\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Set device\n",
        "torch_device = \"cuda\"\n",
        "offload_device = \"cpu\"\n",
        "\n",
        "model_name = \"CompVis/stable-diffusion-v1-4\" #@param {\"type\":\"string\"}\n",
        "\n",
        "if \".ckpt\" in model_name:\n",
        "    !curl https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py > convert.py\n",
        "    !rm -r output-model\n",
        "    repo_id = '/'.join(model_name.split(\"/\")[:2])\n",
        "    filename = '/'.join(model_name.split(\"/\")[2:])\n",
        "    print(repo_id, filename)\n",
        "    compvis_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "\n",
        "    !python convert.py --checkpoint_path \"$compvis_path\" --dump_path \"output-model\"\n",
        "    model_name = \"output-model\"\n",
        "    clear_output(wait=False)\n",
        "    print(\"Model converted and saved at\", model_name)\n",
        "\n",
        "\n",
        "custom_vae = \"\" #@param {type:\"string\"}\n",
        "if custom_vae!=\"\":\n",
        "    vae_model_name = \"/\".join(custom_vae.split(\"/\")[:2])\n",
        "    vae_subfolder = \"/\".join(custom_vae.split(\"/\")[2:])\n",
        "else:\n",
        "    vae_model_name = model_name\n",
        "    vae_subfolder = \"vae\"\n",
        "vae = AutoencoderKL.from_pretrained(vae_model_name, subfolder=vae_subfolder, use_auth_token=True)\n",
        "\n",
        "try:\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\")\n",
        "except:\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    print(\"Could not load CLIP model from repo.\")\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\", use_auth_token=True)\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "vae = vae.half().to(torch_device).eval()\n",
        "text_encoder = text_encoder.half().to(torch_device).eval()\n",
        "unet = unet.half().to(torch_device).eval()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L3to5JkvGjO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Customizations"
      ],
      "metadata": {
        "id": "67Z7qZFf4q6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load textual-inversion concepts from ðŸ¤— hub\n",
        "\n",
        "#@markdown `specific_concepts` can be a list of strings, containing the ids of your concepts (from sd-concepts-library or your own repos, example `[\"sd-concepts-library/my-first-inversion\", \"sd-concepts-library/my-second-inversion\"]` etc.)\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "!pip install wget -q\n",
        "import wget\n",
        "import os\n",
        "import requests\n",
        "from huggingface_hub import hf_hub_download\n",
        "api = HfApi()\n",
        "def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "        loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
        "        \n",
        "        # separate token and the embeds\n",
        "        trained_token = list(loaded_learned_embeds.keys())[0]\n",
        "        embeds = loaded_learned_embeds[trained_token]\n",
        "\n",
        "        # cast to dtype of text_encoder\n",
        "        dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "        embeds.to(dtype)\n",
        "\n",
        "        # add the token in tokenizer\n",
        "        token = token if token is not None else trained_token\n",
        "        num_added_tokens = tokenizer.add_tokens(token)\n",
        "        i = 1\n",
        "        # while(num_added_tokens == 0):\n",
        "        #     print(f\"The tokenizer already contains the token {token}.\")\n",
        "        #     token = f\"{token[:-1]}-{i}>\"\n",
        "        #     print(f\"Attempting to add the token {token}.\")\n",
        "        #     num_added_tokens = tokenizer.add_tokens(token)\n",
        "        #     i+=1\n",
        "        \n",
        "        # resize the token embeddings\n",
        "        text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "        \n",
        "        # get the id for the token and assign the embeds\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "        return token\n",
        "\n",
        "specific_concepts = [\"sd-concepts-library/cat-toy\"] #@param\n",
        "\n",
        "models = []\n",
        "for model in specific_concepts:\n",
        "    model_content = {}\n",
        "    model_content[\"id\"] = model\n",
        "    embeds = (model, \"learned_embeds.bin\")\n",
        "    embeds_file = hf_hub_download(embeds[0], embeds[1])\n",
        "    \n",
        "    token_identifier = (model, \"token_identifier.txt\")\n",
        "    tokens_file = hf_hub_download(token_identifier[0], token_identifier[1])\n",
        "    # print(f\"added {token_name}\")\n",
        "    token_name = open(tokens_file, \"r\").read()\n",
        "    print(\"adding\", token_name)\n",
        "\n",
        "    learned_token = load_learned_embed_in_clip(embeds_file, text_encoder, tokenizer, token_name)\n",
        "    model_content[\"token\"] = learned_token\n",
        "    models.append(model_content)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yRt2hckv4jNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title convert old embedding\n",
        "\n",
        "#@markdown convert original textual-inversion embeds to huggingface-style ones <br> You can run this cell however many times is needed to input all of your inversions.\n",
        "\n",
        "from IPython.display import FileLink\n",
        "import torch\n",
        "\n",
        "input_file = \"\" #@param {\"type\":\"string\"}\n",
        "placeholder_token = \"<my-style>\" #@param {\"type\":\"string\"}\n",
        "\n",
        "def convert_and_load(input_file, placeholder_token):\n",
        "    x = torch.load(input_file, map_location=torch.device('cpu'))\n",
        "\n",
        "    params_dict = {\n",
        "        placeholder_token: torch.tensor(list(x['string_to_param'].items())[0][1])\n",
        "    }\n",
        "    torch.save(params_dict, \"learned_embeds.bin\")\n",
        "    load_learned_embed_in_clip(\"learned_embeds.bin\", text_encoder, tokenizer, placeholder_token)\n",
        "    print(\"loaded\", placeholder_token)\n",
        "\n",
        "if input_file != \"\":\n",
        "    convert_and_load(input_file, placeholder_token)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wcws0yPc7jOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown load a few midjourney styles\n",
        "import gdown\n",
        "gdown.download_folder(url=\"https://drive.google.com/drive/u/9/folders/1whqzuBtiAIo9V12I20I1EVkfE9TLb1hS\", quiet=True)\n",
        "\n",
        "try:\n",
        "    folder = \"/content/midj textual inversion\"\n",
        "    files = [folder+\"/\"+i for i in os.listdir(folder) if \".pt\" in i]\n",
        "except: # WHY DOES IT SOMETIMES DO THIS AND SOMETIMES DO IT THE OTHER WAY????? IM SO ANGRY\n",
        "    folder = \"/content\"\n",
        "    files = [folder+\"/\"+i for i in os.listdir(folder) if \".pt\" in i]\n",
        "names = [\"<\"+i.split(\"/\")[-1].split(\".\")[0]+\">\" for i in files]\n",
        "\n",
        "for i,j in zip(files, names):\n",
        "    convert_and_load(i,j)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eBOTb1d24meL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title set up CLIP related functions\n",
        "import open_clip as clip\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "def get_clip_model(name, pretrained=\"openai\", cutn = 4):\n",
        "    clip_model, _, preprocess = clip.create_model_and_transforms(clip_model_name, pretrained=clip_model_pretrained)\n",
        "    clip_model = clip_model.eval().requires_grad_(False).to(torch_device)\n",
        "    make_cutouts = MakeCutouts(clip_model.visual.image_size if type(clip_model.visual.image_size)!= tuple else clip_model.visual.image_size[0], cutn)\n",
        "    return clip_model, make_cutouts\n",
        "\n",
        "target = None\n",
        "clip_model = None\n",
        "\n",
        "def spherical_distance(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    l = (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2).mean()\n",
        "    return l \n",
        "\n",
        "def clip_loss(x):\n",
        "    global clip_model \n",
        "    global target\n",
        "    global make_cutouts\n",
        "\n",
        "    global unet \n",
        "    global vae\n",
        "\n",
        "    unet = unet.cpu()\n",
        "    vae = vae.cpu()\n",
        "\n",
        "    cutouts = make_cutouts(x)\n",
        "    clip_model = clip_model.half().cuda()\n",
        "    encoding = clip_model.encode_image(cutouts.half().cuda())#.half()\n",
        "    clip_model = clip_model.half().cpu()\n",
        "\n",
        "    loss = spherical_distance(encoding, target)\n",
        "\n",
        "    unet = unet.cuda()\n",
        "    vae = vae.cuda()\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GNuB12h94-v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate"
      ],
      "metadata": {
        "id": "vSqWdS8l4t-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = None\n",
        "#@markdown Main Parameters\n",
        "prompt = \"\\u003Cmidj-strong>\" #@param {type:\"string\"}\n",
        "negative = \"\" #@param {type:\"string\"}\n",
        "width_height = [1920, 1024] #@param\n",
        "width, height = width_height\n",
        "steps = 45 #@param\n",
        "scheduler.set_timesteps(steps)\n",
        "scale = 7.5 #@param\n",
        "batches = 4 #@param\n",
        "attention_slice_size = \"2\" #@param [1, 2, 4, 8, \"None\"]\n",
        "if attention_slice_size != \"None\":\n",
        "    unet.set_attention_slice(int(attention_slice_size))\n",
        "\n",
        "seed = 1 #@param\n",
        "if seed == -1:\n",
        "    seed = random.randint(1, 100_000)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Classifier Guidance\n",
        "\n",
        "clip_model_name = \"ViT-B-32\" #@param {type:\"string\"}\n",
        "clip_model_pretrained = \"laion2b_s34b_b79k\" #@param {type:\"string\"}\n",
        "cutn = 1 #@param\n",
        "\n",
        "clip_text_prompt = \"\" #@param {type:\"string\"}\n",
        "clip_image_prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    import io\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return PImage.open(fd).convert('RGB')\n",
        "    return PImage.open(open(url_or_path, 'rb')).convert('RGB')\n",
        "\n",
        "clip_scale = 0. #@param\n",
        "clamp_max = 0.01 #@param\n",
        "if clip_scale != 0:\n",
        "    clip_model, make_cutouts = get_clip_model(clip_model_name, clip_model_pretrained, cutn)\n",
        "    input_tokens = clip.tokenize(clip_text_prompt).cuda()\n",
        "    clip_model.cuda()\n",
        "    if len(clip_text_prompt) > 1:\n",
        "        with torch.no_grad():\n",
        "            print(\"Creating CLIP Text Target..\")\n",
        "            target = clip_model.encode_text(clip.tokenize(clip_text_prompt).to(torch_device))\n",
        "            target = target.detach().requires_grad_(False)\n",
        "            clip_model.transformer = None # we dont use it, delete to save vram\n",
        "    if len(clip_image_prompt) != \"\":\n",
        "        with torch.no_grad():\n",
        "            if target!=None:\n",
        "                print(\"Creating CLIP Image Target.. (adding)\")\n",
        "                new_target = clip_model.encode_image(make_cutouts(f.to_tensor(fetch(clip_image_prompt)).unsqueeze(0).to(torch_device))).mean(0).unsqueeze(0)\n",
        "                target = target * 0.5 + new_target\n",
        "            else:\n",
        "                print(\"Creating CLIP Image Target.. (new)\")\n",
        "                target = clip_model.encode_image(make_cutouts(f.to_tensor(fetch(clip_image_prompt)).unsqueeze(0).to(torch_device))).mean(0).unsqueeze(0)\n",
        "    else:\n",
        "        raise Exception(\"You need to set a clip_text_prompt\")\n",
        "losses = [\n",
        "    {\n",
        "        \"name\": \"clip_loss\",\n",
        "        \"function\": clip_loss,\n",
        "        \"scale\": clip_scale\n",
        "    }\n",
        "]\n",
        "guidance=False\n",
        "for i in losses:\n",
        "    if i['scale'] != 0:\n",
        "        guidance = True\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Img2Img\n",
        "input_strength = 0. #@param\n",
        "strength = 1 - input_strength\n",
        "input_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def preprocess(image):\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "def latents_to_pil(latents):\n",
        "    # grokking notebook\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents.half())['sample']\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [PImage.fromarray(image) for image in images]\n",
        "    return pil_images\n",
        "def generate(\n",
        "    unet = unet,\n",
        "    vae = vae,\n",
        "    text_encoder = text_encoder,\n",
        "    scheduler = scheduler,\n",
        "    #######################\n",
        "    prompt = prompt,\n",
        "    negative = negative,\n",
        "    width_height = width_height,\n",
        "    width = width,\n",
        "    height = height,\n",
        "    steps = steps,\n",
        "    scale = scale,\n",
        "    ########################\n",
        "    losses = losses,\n",
        "    ######################\n",
        "    strength = strength,\n",
        "    input_url = input_url\n",
        "):\n",
        "    if input_url!=\"\":\n",
        "        with torch.no_grad():\n",
        "            input_pil_image = fetch(input_url).resize(width_height)\n",
        "            init_image = preprocess(input_pil_image)\n",
        "\n",
        "            init_image = init_image.cuda().half()\n",
        "\n",
        "            vae.cuda()\n",
        "            with torch.no_grad():\n",
        "                init_latent_dist = vae.encode(init_image).latent_dist\n",
        "            vae.cpu()\n",
        "\n",
        "            init_latents = init_latent_dist.sample()\n",
        "            init_latents = 0.18215 * init_latents\n",
        "\n",
        "            # get the original timestep using init_timestep\n",
        "            offset = scheduler.config.get(\"steps_offset\", 0)\n",
        "            init_timestep = int(steps * strength) + offset\n",
        "            init_timestep = min(init_timestep, steps)\n",
        "            timesteps = scheduler.timesteps[-init_timestep]\n",
        "            timesteps = torch.tensor([timesteps], device='cuda')\n",
        "\n",
        "            # add noise to latents using the timesteps\n",
        "            noise = torch.randn(init_latents.shape, device='cuda', dtype=torch.half)\n",
        "            latents = scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # I know this is messy and ugly but (at the time of writing) it's required\n",
        "        del timesteps\n",
        "        del init_latent_dist\n",
        "        del init_image\n",
        "        del noise\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_encoder.cuda()\n",
        "        tokens_prompt = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "        tokens_negative = tokenizer(negative, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "        prompt_embedding = text_encoder(tokens_prompt.input_ids.to(torch_device))[0]\n",
        "        negative_embedding = text_encoder(tokens_negative.input_ids.to(torch_device))[0]\n",
        "        text_encoder.cpu()\n",
        "    if input_url==\"\":\n",
        "        latents = torch.randn(1, 4, height // 8, width // 8)\n",
        "        latents = latents.to(torch_device)\n",
        "        latents = latents * scheduler.init_noise_sigma\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        if input_url!=\"\":\n",
        "            t_start = max(steps - init_timestep + offset, 0)\n",
        "            timesteps = scheduler.timesteps[t_start:].cuda()\n",
        "            pbar = tqdm(enumerate(timesteps), total=len(timesteps))\n",
        "        else:\n",
        "            pbar = tqdm(enumerate(scheduler.timesteps), total=steps)\n",
        "        for i, t in pbar:\n",
        "            sigma = scheduler.sigmas[i]\n",
        "            model_input = scheduler.scale_model_input(latents, t)\n",
        "            conditional_input = model_input.clone().detach()\n",
        "            unconditional_input = model_input.clone().detach()\n",
        "\n",
        "            # split to save memory\n",
        "            if guidance:\n",
        "                with torch.no_grad():\n",
        "                    unconditional_noise_pred = unet(unconditional_input, t, encoder_hidden_states = negative_embedding)['sample']\n",
        "                conditional_input = conditional_input.requires_grad_()\n",
        "                conditional_noise_pred = unet(conditional_input, t, encoder_hidden_states = prompt_embedding)['sample']\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    unconditional_noise_pred = unet(unconditional_input, t, encoder_hidden_states = negative_embedding)['sample']\n",
        "                    conditional_noise_pred = unet(conditional_input, t, encoder_hidden_states = prompt_embedding)['sample']\n",
        "            # cfg\n",
        "            noise_pred = unconditional_noise_pred + scale * (conditional_noise_pred - unconditional_noise_pred)\n",
        "            if guidance:\n",
        "                lats = latents.detach().requires_grad_()\n",
        "                lats_x0 = latents - sigma * noise_pred\n",
        "                # move unet to cpu so theres more room on vram to decode image\n",
        "                # just in case (required at the time of writing for some reason)\n",
        "                unet.cpu()\n",
        "                vae.cuda()\n",
        "                denoised_images = vae.decode((1 / 0.18215) * lats_x0).sample / 2 + .5\n",
        "                \n",
        "                loss = 0\n",
        "                for loss_object in losses:\n",
        "                    if loss_object['scale'] != 0:\n",
        "                        loss = loss + loss_object['function'](denoised_images).half() * loss_object['scale']\n",
        "\n",
        "                # move it back\n",
        "                unet.cuda()\n",
        "                if clip_model != None:\n",
        "                    clip_model.cuda().half()\n",
        "                cond_grad = -torch.autograd.grad(loss, conditional_input)[0]\n",
        "                if clip_model != None:\n",
        "                    clip_model.cpu().half()\n",
        "                cond_grad = torch.nan_to_num(cond_grad)\n",
        "                magnitude = cond_grad.square().mean().sqrt()\n",
        "                cond_grad = cond_grad * magnitude.clamp(max=clamp_max) / magnitude\n",
        "                cond_grad = torch.nan_to_num(cond_grad)\n",
        "\n",
        "                latents = latents.detach() + cond_grad.detach() * sigma**2  \n",
        "\n",
        "                # hrhrghrmgbbndmvhxbms,jgfsj;\n",
        "                del cond_grad\n",
        "                del loss \n",
        "                del lats_x0 \n",
        "                del lats \n",
        "            latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "    vae.cuda()\n",
        "    latents_to_pil(latents)[0].save(\"temp.png\")\n",
        "    del latents\n",
        "    display(Image(\"temp.png\"))\n",
        "    vae.cpu()\n",
        "\n",
        "for _b in range(batches):\n",
        "    generate()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0dnFtL7QHA9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸ””\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f6TIZLzX9bDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Models:\n",
        "```\n",
        "[('ViT-B-32', 'openai'),\n",
        "('ViT-B-32', 'laion400m_e31'),\n",
        "('ViT-B-32', 'laion400m_e32'),\n",
        "('ViT-B-32', 'laion2b_e16'),\n",
        "('ViT-B-32', 'laion2b_s34b_b79k'),\n",
        "('ViT-B-32-quickgelu', 'openai'),\n",
        "('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
        "('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
        "\n",
        "# these ones dont fit on free colab (to my knowledge):\n",
        "('ViT-B-16', 'openai'),\n",
        "('ViT-B-16', 'laion400m_e31'),\n",
        "('ViT-B-16', 'laion400m_e32'),\n",
        "('ViT-B-16-plus-240', 'laion400m_e31'),\n",
        "('ViT-B-16-plus-240', 'laion400m_e32'),\n",
        "('ViT-L-14', 'openai'),\n",
        "('ViT-L-14', 'laion400m_e31'),\n",
        "('ViT-L-14', 'laion400m_e32'),\n",
        "('ViT-L-14', 'laion2b_s32b_b82k'),\n",
        "('ViT-L-14-336', 'openai'),\n",
        "('ViT-H-14', 'laion2b_s32b_b79k'),\n",
        "('ViT-g-14', 'laion2b_s12b_b42k')]\n",
        "```"
      ],
      "metadata": {
        "id": "aWo8ROBQIGnz"
      }
    }
  ]
}